{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uua1wvw7iQB4"
   },
   "source": [
    "![](logo1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMnWC3LuifEi"
   },
   "source": [
    "# **shAI Training 2023 | Level 1**\n",
    "\n",
    "## Task #8 (End-to-End ML Project {part_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rbe_ZnKi-uY"
   },
   "source": [
    "## Welcome to the exercises for reviewing second part of end to end ML project.\n",
    "**Make sure that you read and understand ch2 from the hands-on ML book (page 72 to the end of the chapter ) before start with this notebook.**\n",
    "\n",
    "**If you stuck with anything reread that part from the book and feel free to ask about anything in the messenger group as you go along.**\n",
    "\n",
    " ## Good Luck : )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAbRZ0fwfOb4"
   },
   "source": [
    "## first run the following cell for the first part of the project to continue your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q3v160SJfL7U"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ObbhNRgSfu6_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "   csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "   return pd.read_csv(csv_path)\n",
    "\n",
    "fetch_housing_data()\n",
    "housing = load_housing_data()\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
    "    list(housing.columns).index(col)\n",
    "    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "housing = train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = train_set[\"median_house_value\"].copy()\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    " ('imputer', SimpleImputer(strategy=\"median\")),\n",
    " ('attribs_adder', CombinedAttributesAdder()),\n",
    " ('std_scaler', StandardScaler())])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    " (\"num\", num_pipeline, num_attribs),\n",
    " (\"cat\", OneHotEncoder(), cat_attribs)])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa6vPfm6jxsF"
   },
   "source": [
    "# 1- Select and Train a Model\n",
    "\n",
    "# Let’s first train a LinearRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JCl0ZYDRjGz_"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression().fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nDxOY7GmTNc"
   },
   "source": [
    "# First try it out on a few instances from the training set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7Lx7VQm7pwSQ"
   },
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BU-ynaaIpYHO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [181746.54359616 290558.74973505 244957.50017771 146498.51061398\n",
      " 163230.42393939]\n",
      "\n",
      "label: [103000.0, 382100.0, 172600.0, 93400.0, 96500.0]\n"
     ]
    }
   ],
   "source": [
    "some_data_prepared = full_pipeline.transform(some_data) \n",
    "\n",
    "print('Prediction:', lin_reg.predict(some_data_prepared))\n",
    "print()\n",
    "print('label:', list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjmwxoU-qFnb"
   },
   "source": [
    "# measure this regression model’s RMSE on the whole training set\n",
    "* sing Scikit-Learn’s mean_squared_error() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rczx22dFqRMc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aCYZh9ExqWMJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67593.20745775253"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prediction = lin_reg.predict(housing_prepared)\n",
    "\n",
    "lin_mse  = mean_squared_error(housing_prediction, housing_labels)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLsKfuQpcfyx"
   },
   "source": [
    "# judge on the RMSE result for this model\n",
    "write down your answar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnBVcR-MeFqa"
   },
   "source": [
    "The error is exceedingly large at **67593**, far surpassing the range of house sales between **120,000** and **256,000**. This discrepancy is primarily attributed to overfitting, indicating that the current model is too simplistic. Hence, it's advisable to opt for a more complex model to better capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vImNak3CqqFo"
   },
   "source": [
    "# Let’s train a Decision Tree Regressor model\n",
    "## more powerful model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8syfCBveqY2q"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vrUPZzBhq-do"
   },
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor().fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRhxYj0Aq9op"
   },
   "source": [
    "# Now evaluate the model on the training set\n",
    "* using Scikit-Learn’s mean_squared_error() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DYCxUSCkrNIY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prediction = tree_reg.predict(housing_prepared)\n",
    "\n",
    "tree_mse  = mean_squared_error(housing_prediction, housing_labels)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSxXI9b8iZPs"
   },
   "source": [
    "# Explaine this result\n",
    "write down your answar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVSMQ7kbiZSi"
   },
   "source": [
    "The error is 0, yet the model isn't perfect. This discrepancy suggests that the model is overfit because we evaluate it on the training data. The model appears to be excessively complex, resulting in it memorizing the training data rather than learning from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj7b4zSPrdyH"
   },
   "source": [
    "# Evaluation Using Cross-Validation\n",
    "\n",
    "1-split the training set into 10 distinct subsets then train and evaluate the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JmNrgsBrwIe3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yXNPsWjcwMd_"
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqReIY3urLP8"
   },
   "source": [
    "2- display the resultant scores and calculate its Mean and Standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1g8jIq-6raVF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [65078.25891532 70696.02750088 69317.03471236 71590.76310531\n",
      " 73540.25698882 67980.28180135 67155.89636265 68593.47233815\n",
      " 67226.35976615 70788.91383135]\n",
      "\n",
      "Mean: 69196.72653223416\n",
      "\n",
      "Standard deviation: 2371.663291454293\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print()\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print()\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "display_scores(tree_rmse_scores)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6paGk_hsGGY"
   },
   "source": [
    "3-repaet the same steps to compute the same scores for the Linear Regression  model\n",
    "\n",
    "*notice the difference between the results of the two models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ol3C6DmusWfx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [65000.67382615 70960.56056304 67122.63935124 66089.63153865\n",
      " 68402.54686442 65266.34735288 65218.78174481 68525.46981754\n",
      " 72739.87555996 68957.34111906]\n",
      "\n",
      "Mean: 67828.38677377408\n",
      "\n",
      "Standard deviation: 2468.0913950652284\n"
     ]
    }
   ],
   "source": [
    "scores= cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdQ85uTEtDy1"
   },
   "source": [
    "## Let’s train one last model the RandomForestRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "O1PPFq5TtdDP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18433.4121948378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor().fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_prediction = forest_reg.predict(housing_prepared)\n",
    "\n",
    "forest_mse  = mean_squared_error(housing_prediction, housing_labels)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "\n",
    "print(forest_rmse)\n",
    "\n",
    "scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSxaBthCtw93"
   },
   "source": [
    "# repeat the same steps to compute the same scores its Mean and Standard deviation for the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AAc2MOQwt2lC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [47211.27972361 51336.73936059 49640.52536047 51775.79330693\n",
      " 52407.94507835 47420.22663188 47481.30874502 50588.73395716\n",
      " 49234.67655459 49980.91663789]\n",
      "\n",
      "Mean: 49707.81453564808\n",
      "\n",
      "Standard deviation: 1781.08677273244\n"
     ]
    }
   ],
   "source": [
    "fore_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "display_scores(fore_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn2u9DOxvE5S"
   },
   "source": [
    "# Save every model you experiment with\n",
    "*using the joblib library*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mWyIi3mtva85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(lin_reg, \"linear_regression.pkl\")\n",
    "joblib.dump(tree_reg, \"decision_tree.pkl\")\n",
    "joblib.dump(forest_reg, \"random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIC6O-h0wOBJ"
   },
   "source": [
    "## now you have a shortlist of promising models. You now need to\n",
    "## fine-tune them!\n",
    "# Fine-Tune Your Model\n",
    "\n",
    "## 1- Grid Search\n",
    "## evaluate all the possible combinations of hyperparameter values for the RandomForestRegressor\n",
    "*It may take a long time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y8Wqd-Pix3Sm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qhDCrx0Y0ocN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    " {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    " {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    " ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv = 5, scoring = \"neg_mean_squared_error\", return_train_score = True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjRCrlqEyH1A"
   },
   "source": [
    "# Analyze the Best Models and Their Errors\n",
    "1-indicate the relative importance of each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "y2MkCD1Byh9F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.71945705e-02, 6.11893135e-02, 4.32442666e-02, 1.52750568e-02,\n",
       "       1.37113604e-02, 1.48345991e-02, 1.33369213e-02, 3.73376289e-01,\n",
       "       3.94745130e-02, 1.14241628e-01, 7.03246705e-02, 8.84838709e-03,\n",
       "       1.55804835e-01, 2.13966461e-04, 4.72486837e-03, 4.20475352e-03])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b01L7mUm1xTV"
   },
   "source": [
    "2-display these importance scores next to their corresponding attribute names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dau43zXt14i7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.37337628897801783, 'median_income'),\n",
       " (0.15580483544091195, 'INLAND'),\n",
       " (0.11424162836515847, 'pop_per_hhold'),\n",
       " (0.0703246705384116, 'bedrooms_per_room'),\n",
       " (0.06719457054146899, 'longitude'),\n",
       " (0.061189313480718675, 'latitude'),\n",
       " (0.04324426659610013, 'housing_median_age'),\n",
       " (0.03947451302988654, 'rooms_per_hhold'),\n",
       " (0.015275056808788932, 'total_rooms'),\n",
       " (0.014834599086150537, 'population'),\n",
       " (0.013711360393585038, 'total_bedrooms'),\n",
       " (0.013336921289840504, 'households'),\n",
       " (0.008848387093206666, '<1H OCEAN'),\n",
       " (0.004724868373348916, 'NEAR BAY'),\n",
       " (0.004204753523340697, 'NEAR OCEAN'),\n",
       " (0.00021396646106457387, 'ISLAND')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_attribs       = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder         = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attribues           = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "\n",
    "sorted(zip(feature_importances, attribues), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esOPiD6Pyice"
   },
   "source": [
    "## Now is the time to evaluate the final model on the test set.\n",
    "# Evaluate Your System on the Test Set\n",
    "\n",
    "1-get the predictors and the labels from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZrmGwOEyykad"
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = test_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhkKu23G2yNd"
   },
   "source": [
    "2-run your full_pipeline to transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VBfW1WG823TE"
   },
   "outputs": [],
   "source": [
    "X_test_prepared = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNVnMSJy28xt"
   },
   "source": [
    "3-evaluate the final model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "HrcgAUoy2_tc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49924.83143250667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_prediction)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYBxgnsx3Ipr"
   },
   "source": [
    "# compute a 95% confidence interval for the generalization error\n",
    "*using scipy.stats.t.interval():*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ngWpgPrE3NaS"
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6RFaMou83WBY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47747.38671616, 52011.19734159])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors = (final_prediction - y_test) ** 2\n",
    "\n",
    "np.sqrt(stats.t.interval(.95, len(squared_errors) - 1,\n",
    "        loc=squared_errors.mean(),\n",
    "        scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTsy6N8Uytpo"
   },
   "source": [
    "# Great Job!\n",
    "# #shAI_Club"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
